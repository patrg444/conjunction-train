#!/usr/bin/env python3
"""
Script to generate visual comparisons of model validation accuracies.
This script reads the JSON data generated by extract_all_models_val_accuracy.py
and creates comparative visualizations.

Usage:
    python compare_model_accuracies.py [--top N] [--min-epochs N] [--output filename.png]

Options:
    --top N          Show only the top N models by best validation accuracy (default: 5)
    --min-epochs N   Only include models with at least N epochs of training (default: 10)
    --output         Output filename for the comparison chart (default: model_comparison.png)
"""

import json
import os
import argparse
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import PercentFormatter
import pandas as pd

def load_model_data(json_file="model_validation_accuracy.json"):
    """Load model data from the JSON file"""
    if not os.path.exists(json_file):
        print(f"Error: JSON file {json_file} not found.")
        print("Please run extract_all_models_val_accuracy.py first.")
        return None
    
    with open(json_file, 'r') as f:
        model_data = json.load(f)
    
    return model_data

def filter_models(model_data, top_n=5, min_epochs=10):
    """Filter models based on criteria"""
    # Filter by minimum epochs
    filtered_data = {
        name: data for name, data in model_data.items() 
        if data['summary']['epochs_completed'] >= min_epochs
    }
    
    # Sort by best validation accuracy
    sorted_models = sorted(
        filtered_data.items(),
        key=lambda x: x[1]['summary']['best_val_accuracy'] or 0,
        reverse=True
    )
    
    # Take top N
    return sorted_models[:top_n]

def plot_accuracy_comparison(sorted_models, output_file="model_comparison.png"):
    """Generate a comparison plot of model validation accuracies"""
    plt.figure(figsize=(12, 8))
    
    # Plot each model's validation accuracy
    for i, (name, data) in enumerate(sorted_models):
        epochs = data['metrics']['epoch']
        val_acc = data['metrics']['val_accuracy']
        
        # Only include complete data (some models might have partial data)
        valid_indices = [i for i, e in enumerate(epochs) if i < len(val_acc)]
        valid_epochs = [epochs[i] for i in valid_indices]
        valid_acc = [val_acc[i] for i in valid_indices]
        
        plt.plot(valid_epochs, valid_acc, 
                 marker='o', markersize=3, 
                 label=f"{name} (Best: {max(valid_acc):.4f})")
    
    # Add gridlines for readability
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Set labels and title
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Validation Accuracy', fontsize=12)
    plt.title('Model Validation Accuracy Comparison', fontsize=14)
    
    # Format y-axis as percentage
    plt.gca().yaxis.set_major_formatter(PercentFormatter(1.0))
    
    # Add legend in a good position
    plt.legend(loc='lower right', fontsize=10)
    
    # Set y-axis to a reasonable range
    plt.ylim(0.5, 1.0)  # 50% to 100%
    
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"Comparison chart saved to {output_file}")
    
    return output_file

def create_performance_table(sorted_models, output_file="model_performance_table.csv"):
    """Create a detailed CSV performance table for the models"""
    rows = []
    
    for name, data in sorted_models:
        summary = data['summary']
        
        # Add to rows
        rows.append({
            'Model': name,
            'Best Val Accuracy': f"{summary['best_val_accuracy']:.4f}",
            'Best Epoch': summary['best_epoch'],
            'Last Val Accuracy': f"{summary['latest_val_accuracy']:.4f}",
            'Total Epochs': summary['epochs_completed'],
            'Trend': f"{summary.get('recent_trend', 0):.6f}" if summary.get('recent_trend') else "N/A"
        })
    
    # Convert to DataFrame
    df = pd.DataFrame(rows)
    
    # Save to CSV
    df.to_csv(output_file, index=False)
    print(f"Performance table saved to {output_file}")
    
    return df

def plot_final_accuracies(sorted_models, output_file="final_accuracies.png"):
    """Generate a bar chart of final best validation accuracies"""
    plt.figure(figsize=(10, 6))
    
    names = [name for name, _ in sorted_models]
    best_accuracies = [data['summary']['best_val_accuracy'] for _, data in sorted_models]
    
    # Create bar chart
    bars = plt.bar(names, best_accuracies, color='skyblue')
    
    # Add data labels above each bar
    for bar, acc in zip(bars, best_accuracies):
        plt.text(
            bar.get_x() + bar.get_width()/2,
            acc + 0.005,
            f"{acc:.4f}",
            ha='center', va='bottom',
            rotation=90, fontsize=9
        )
    
    # Set labels and title
    plt.xlabel('Model', fontsize=12)
    plt.ylabel('Best Validation Accuracy', fontsize=12)
    plt.title('Best Validation Accuracy by Model', fontsize=14)
    
    # Format y-axis as percentage
    plt.gca().yaxis.set_major_formatter(PercentFormatter(1.0))
    
    # Set y-axis to start at a reasonable value
    min_acc = min(best_accuracies) * 0.95
    plt.ylim(min_acc, 1.0)
    
    # Rotate x-axis labels for readability
    plt.xticks(rotation=45, ha='right', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"Final accuracies chart saved to {output_file}")
    
    return output_file

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Compare model validation accuracies')
    parser.add_argument('--top', type=int, default=5, help='Show only the top N models')
    parser.add_argument('--min-epochs', type=int, default=10, 
                        help='Only include models with at least N epochs of training')
    parser.add_argument('--output', type=str, default='model_comparison.png',
                        help='Output filename for the comparison chart')
    args = parser.parse_args()
    
    # Load model data
    model_data = load_model_data()
    if not model_data:
        return
    
    # Filter models
    sorted_models = filter_models(model_data, args.top, args.min_epochs)
    if not sorted_models:
        print(f"No models found with at least {args.min_epochs} epochs of training.")
        return
    
    # Print info
    print(f"Generating comparison for top {len(sorted_models)} models (min {args.min_epochs} epochs):")
    for i, (name, data) in enumerate(sorted_models, 1):
        summary = data['summary']
        print(f"{i}. {name}: {summary['best_val_accuracy']:.4f} (Epoch {summary['best_epoch']})")
    
    # Create visualizations
    plot_accuracy_comparison(sorted_models, args.output)
    plot_final_accuracies(sorted_models, "final_accuracies.png")
    create_performance_table(sorted_models, "model_performance_table.csv")
    
    print("\nAll comparison visualizations generated successfully!")
    print("You can now examine the comparison charts and performance table.")

if __name__ == "__main__":
    main()
