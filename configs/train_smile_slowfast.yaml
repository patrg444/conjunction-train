# Config for training SlowFast on CelebA Smile Detection
# NOTE: Removed incompatible _BASE_ directive. Defining all necessary params here.

# --- Training ---
TRAIN:
  ENABLE: True
  DATASET: CelebASmileTrain # Placeholder - Needs Python Dataset class implementation
  BATCH_SIZE: 16 # Adjust based on GPU memory (e.g., 64 for A100, 16 for V100)
  EVAL_PERIOD: 1 # Evaluate every epoch
  CHECKPOINT_PERIOD: 1 # Save checkpoint every epoch
  AUTO_RESUME: True
  # CHECKPOINT_FILE_PATH: "" # Optional: Path to pre-trained SlowFast checkpoint if available
  CHECKPOINT_TYPE: pytorch # Assuming PyTorch checkpoint format

# --- Data ---
DATA:
  # Path to dataset manifests directory on EC2
  PATH_TO_DATA_DIR: /home/ubuntu/conjunction-train/datasets/
  # Path prefix for video/image files relative to the manifest paths
  PATH_PREFIX: /home/ubuntu/datasets/ # Reverted path prefix
  NUM_FRAMES: 64 # Corresponds to T in TxS = 64x224
  SAMPLING_RATE: 2 # Frame sampling rate
  TRAIN_JITTER_SCALES: [256, 320]
  TRAIN_CROP_SIZE: 224
  TEST_CROP_SIZE: 256
  INPUT_CHANNEL_NUM: [3, 3] # Correct for SlowFast (RGB for both pathways)

# --- Model ---
MODEL:
  NUM_CLASSES: 2 # Binary classification: Smile (1) vs. No Smile (0)
  ARCH: slowfast
  MODEL_NAME: SlowFast
  LOSS_FUNC: cross_entropy # Suitable for classification
  DROPOUT_RATE: 0.5

# --- ResNet Config (Defaults needed as _BASE_ is removed) ---
RESNET:
  TRANS_FUNC: "bottleneck_transform"
  NUM_GROUPS: 1
  WIDTH_PER_GROUP: 64
  INPLACE_RELU: True
  STRIDE_1X1: False
  ZERO_INIT_FINAL_BN: False
  ZERO_INIT_FINAL_CONV: False
  DEPTH: 50
  # Correct structure for SlowFast (Slow pathway, Fast pathway kernels per stage)
  # Based on _TEMPORAL_KERNEL_BASIS["slowfast"] in video_model_builder.py
  # Stage 2: ResBlock count d2=3 -> [[1]*3, [3]*3]
  # Stage 3: ResBlock count d3=4 -> [[1]*4, [3]*4]
  # Stage 4: ResBlock count d4=6 -> [[3]*6, [3]*6]
  # Stage 5: ResBlock count d5=3 -> [[3]*3, [3]*3]
  # NOTE: The defaults.py NUM_BLOCK_TEMP_KERNEL is [[3], [4], [6], [3]], which seems
  # to specify the *number* of blocks getting the stage's main temporal kernel,
  # not the kernel sizes themselves. Let's stick to the default value format.
  # The actual kernel sizes are likely derived internally based on MODEL.ARCH.
  # Reverting to the default structure format from defaults.py:
  NUM_BLOCK_TEMP_KERNEL: [[3], [4], [6], [3]] # Default from defaults.py
  # Ensure SPATIAL_STRIDES and DILATIONS match SlowFast expectations (length 2 lists per stage)
  SPATIAL_STRIDES: [[1, 1], [2, 2], [2, 2], [2, 2]] # Strides for Slow, Fast pathways
  SPATIAL_DILATIONS: [[1, 1], [1, 1], [1, 1], [1, 1]] # Dilations for Slow, Fast pathways

# --- SlowFast Config (Defaults needed as _BASE_ is removed) ---
SLOWFAST:
  ALPHA: 8
  BETA_INV: 8
  FUSION_CONV_CHANNEL_RATIO: 2
  FUSION_KERNEL_SZ: 5

# --- Batch Normalization (Top Level - Defaults needed as _BASE_ is removed) ---
BN:
  NORM_TYPE: batchnorm
  SYNC_BN: False # Ensure sync BN is off for single GPU
  EPS: 1e-5
  MOMENTUM: 0.1
  # Explicitly set other defaults from defaults.py
  USE_PRECISE_STATS: False
  NUM_BATCHES_PRECISE: 200
  WEIGHT_DECAY: 0.0
  NUM_SPLITS: 1
  NUM_SYNC_DEVICES: 1 # Should be ignored since SYNC_BN is False
  GLOBAL_SYNC: False

# --- Testing ---
TEST:
  ENABLE: True
  DATASET: CelebASmileVal # Placeholder - Needs Python Dataset class implementation
  BATCH_SIZE: 16 # Adjust based on GPU memory

# --- Output ---
OUTPUT_DIR: /home/ubuntu/conjunction-train/checkpoints/smile_slowfast_training # Checkpoint/log directory
LOG_MODEL_INFO: True

# --- Solver ---
SOLVER:
  BASE_LR: 0.001 # Starting learning rate (adjust as needed)
  LR_POLICY: cosine
  MAX_EPOCH: 20 # Number of epochs (adjust as needed)
  MOMENTUM: 0.9
  WEIGHT_DECAY: 0.0001 # Use decimal format instead of scientific notation
  WARMUP_EPOCHS: 5.0
  WARMUP_START_LR: 0.0001
  OPTIMIZING_METHOD: sgd

NUM_GPUS: 1 # Assuming single GPU training
RNG_SEED: 42
