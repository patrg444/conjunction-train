adam_epsilon: 1.0e-08
class_weights: !!python/object/apply:torch._utils._rebuild_tensor_v2
- !!python/object/apply:torch.storage._load_from_bytes
  - !!binary |
    gAKKCmz8nEb5IGqoUBkugAJN6QMugAJ9cQAoWBAAAABwcm90b2NvbF92ZXJzaW9ucQFN6QNYDQAA
    AGxpdHRsZV9lbmRpYW5xAohYCgAAAHR5cGVfc2l6ZXNxA31xBChYBQAAAHNob3J0cQVLAlgDAAAA
    aW50cQZLBFgEAAAAbG9uZ3EHSwR1dS6AAihYBwAAAHN0b3JhZ2VxAGN0b3JjaApGbG9hdFN0b3Jh
    Z2UKcQFYCgAAADQ1OTM3ODMxODRxAlgDAAAAY3B1cQNLAk50cQRRLoACXXEAWAoAAAA0NTkzNzgz
    MTg0cQFhLgIAAAAAAAAAAACAPwAAgD8=
- 0
- !!python/tuple
  - 2
- !!python/tuple
  - 1
- false
- !!python/object/apply:collections.OrderedDict
  - []
dropout: 0.1
learning_rate: 2.0e-05
model_name: xlm-roberta-large
num_classes: 2
scheduler_type: cosine
total_steps: 15
warmup_steps: 1
weight_decay: 0.01
