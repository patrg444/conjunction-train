# Dataset Input File Locations

This document outlines the identified locations for input data files and directories used by various models in this project, based on file listings and script analysis. Paths are relative to the project root (`/Users/patrickgloria/conjunction-train`).

## Manifest Files (CSV/TSV/JSON)

These files typically define dataset splits and point to raw data or features.

*   `./splits/train.csv` (Used by Hubert SER (`ser_hubert/data_module.py`), Fusion (`scripts/validate_fusion_data.py`, `scripts/train_fusion_model.py`))
*   `./splits/val.csv` (Used by Hubert SER, Fusion)
*   `./splits/test.csv` (Used by Hubert SER, Fusion)
*   `./splits/crema_d_train.csv` (Used by Fusion)
*   `./splits/crema_d_val.csv` (Used by Fusion)
*   `./data/audio_manifest.tsv`
*   `./data/audio_manifest_remapped.tsv`
*   `./datasets_raw/manifests/laughter_v1.csv` (Default input path in `scripts/train_audio_pooling_lstm_with_laughter.py`)
*   `./data/class_counts.json` (Potentially used for class weighting)
*   `/home/ubuntu/datasets/video_manifest.csv` (Default output in `scripts/generate_video_manifest.py`, likely an EC2 path, potentially used as input later)

## Raw Data Directories

These directories likely contain the original audio/video files referenced by paths within the manifest files.

*   `./data/CREMA-D/`
*   `./data/RAVDESS/`
*   *(Note: `./datasets_raw/raw/` was found to be empty, suggesting raw data is stored elsewhere, possibly in the `./data/` subdirectories above or requires external download/placement).*

## Precomputed Feature Directories & Files

These locations store features extracted from the raw data.

*   **Extracted Hubert Embeddings (Expected Location: `./splits/`)**
    *   These `.npz` files contain the precomputed Hubert embeddings corresponding to the manifest CSV files.
    *   They are generated by `scripts/rebuild_hubert_embeddings.py` and used as input by models like the Fusion model (`scripts/validate_fusion_data.py`, `scripts/train_fusion_model.py`).
    *   `./splits/train_embeddings.npz`
    *   `./splits/val_embeddings.npz`
    *   `./splits/test_embeddings.npz`
    *   `./splits/crema_d_train_embeddings.npz`
    *   `./splits/crema_d_val_embeddings.npz`
*   **CNN Audio Features (Expected Location: `./data/`)**
    *   `./data/crema_d_features_cnn_audio/`
    *   `./data/crema_d_features_cnn_fixed/`
    *   `./data/ravdess_features_cnn_audio/`
    *   `./data/ravdess_features_cnn_fixed/`
*   **Spectrogram Features (Expected Location: `./data/`)**
    *   `./data/crema_d_features_spectrogram/`
    *   `./data/ravdess_features_spectrogram/`
*   **Wav2Vec Features (Expected Location: `./data/`)**
    *   `./data/wav2vec/` (Also seen referenced via symlinks on EC2, e.g., `/data/wav2vec_features/`, `/data/wav2vec_crema_d/`)
*   **FaceNet Features (Expected Location: Project Root)**
    *   `./crema_d_features_facenet/`

## Normalization Statistics Files

These files store calculated statistics (e.g., mean, std) used to normalize features.

*   `./audio_mean.npy` (Located in project root)
*   `./audio_std.npy` (Located in project root)
*   `models/*normalization_stats.pkl` (Pattern seen in monitoring scripts, likely within specific model output directories)

## Configuration Files (Potentially Relevant)

These files might contain settings that influence data loading or feature extraction.

*   `./config/eGeMAPSv02.conf` (OpenSMILE configuration, likely used for feature extraction)
*   `./config/slowfast_face.yaml` (SlowFast model configuration, defines splits "train", "val" but not the base data path)

## EC2 Specific Paths (Commonly Seen in Scripts)

Many deployment and training scripts reference paths specific to an EC2 environment, often under `/home/ubuntu/`. These are distinct from the local project structure and are crucial for understanding remote execution.

*   Raw Datasets:
    *   `/home/ubuntu/datasets/ravdess_videos/`
    *   `/home/ubuntu/datasets/crema_d_videos/`
*   Manifests:
    *   `/home/ubuntu/datasets/video_manifest.csv`
*   Feature Storage (Examples):
    *   `/data/wav2vec_features/` (Often used as a target for symlinks)
    *   `/data/wav2vec_crema_d/` (Often used as a target for symlinks)
    *   `/home/ubuntu/audio_emotion/models/wav2vec/` (Seen as a source for symlinks)
    *   `/home/ubuntu/emotion_project/wav2vec_features/` (Seen as a source/target for symlinks)
    *   `/home/ubuntu/features/wav2vec/`
*   Project/Output Directories (Examples):
    *   `/home/ubuntu/emotion_project/`
    *   `/home/ubuntu/audio_emotion/`
    *   `/home/ubuntu/emotion_slowfast/`
    *   `/home/ubuntu/emotion_fusion_output/`

## Model Checkpoints/Outputs

While primarily outputs, saved model checkpoints (`.pt`, `.h5`, `.weights.h5`) become inputs for inference, evaluation, or continued training.

*   Common Directories: `models/`, `checkpoints/` (both locally and on EC2, e.g., `/home/ubuntu/emotion_project/checkpoints/`)
*   Specific paths are often dynamically generated based on timestamps or experiment names within scripts (e.g., `checkpoints/MODEL_NAME_TIMESTAMP_best.pt`).
*   Download scripts (`download_*.sh`) often retrieve models from these directories on EC2 instances.

## External Dataset Downloads

Raw datasets like CREMA-D and RAVDESS are typically not included directly in the repository.

*   They need to be downloaded separately from their official sources.
*   Placement: Based on script references, they are expected to be placed in directories like `./data/CREMA-D/` and `./data/RAVDESS/` locally, or `/home/ubuntu/datasets/crema_d_videos/` and `/home/ubuntu/datasets/ravdess_videos/` on EC2. Upload scripts like `upload_datasets_rsync.sh` facilitate this transfer.

**Note:** This list is compiled from directory scans and static analysis of scripts. The exact files and paths used during runtime might depend on command-line arguments, environment variables, or dynamic configurations not fully captured here. Always refer to the specific training/processing scripts and their execution context for definitive input paths.
